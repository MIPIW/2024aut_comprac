{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, GPT2LMHeadModel, AutoConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "import evaluate\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os, json\n",
    "from transformers import set_seed\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "# set seed from transformers\n",
    "set_seed(42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unused -> instead, use gpt2 config model from huggingface\n",
    "# class PriceExpector(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "#         super(PriceExpector, self).__init__()\n",
    "#         self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "#         self.fc1 = nn.Linear(hidden_dim, hidden_dim, bias = True)\n",
    "#         self.fc2 = nn.Linear(hidden_dim, output_dim, bias = True)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         x = self.embedding(inputs['input_ids'])\n",
    "\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer setter\n",
    "def set_trainer(model, tokenizer, dataset, output_dir, args):\n",
    "    \n",
    "    # load metrics\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "\n",
    "    # set metric function\n",
    "    def metric(eval_pred, func):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis = -1) # (batch, sequence lenagh, hidden_state)\n",
    "        # if tokens are not the PAD token\n",
    "        filters = labels != -100\n",
    "\n",
    "        predictions = predictions[filters]\n",
    "        labels = labels[filters]\n",
    "\n",
    "        # 'func' will be 'accuracy' above\n",
    "        return func.compute(predictions = predictions, references = labels)\n",
    "    \n",
    "    # tokenizing, using 4 processor \n",
    "    def tokenize_func(examples):\n",
    "        return tokenizer(examples['0'], truncation=True, padding=True)\n",
    "    training_data = dataset['train'].map(tokenize_func, batched=True, num_proc = 4)\n",
    "    valid_data = dataset['valid'].map(tokenize_func, batched=True, num_proc = 4)\n",
    "    test_data = dataset['test'].map(tokenize_func, batched=True, num_proc = 4)\n",
    "\n",
    "    # remove original columns, leaving 'input_ids' 'attention_mask' '그리고 하나 뭐였지' only\n",
    "    training_data = training_data.remove_columns(['0'])\n",
    "    valid_data = valid_data.remove_columns(['0'])\n",
    "    test_data = test_data.remove_columns(['0'])\n",
    "\n",
    "    # make output directory of current time, where weights are being saved. \n",
    "    od = output_dir + os.sep + datetime.strftime(datetime.now(), \"%m-%d-%H-%M-%S\")\n",
    "    try: os.mkdir(od)\n",
    "    except: pass\n",
    "\n",
    "    # maximum batch, other hyperparameters except learning rate and weight decay are of defaults\n",
    "    trainingarguments = TrainingArguments(\n",
    "        do_train = True,    \n",
    "        output_dir = od,                         \n",
    "        evaluation_strategy = \"steps\", # necessary: change to step\n",
    "        save_strategy = \"steps\",                         \n",
    "        eval_steps = 50, # necessary: set step\n",
    "        save_steps = 50,\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = True, # necessary: EarlyStoppingCallBack하려면 True여야 함\n",
    "        metric_for_best_model = \"accuracy\",\n",
    "        greater_is_better = True, # necessary: higher metric results better performance # default = True when metric_for_best_model is set\n",
    "        num_train_epochs = 3,\n",
    "        seed = 42,\n",
    "        per_device_train_batch_size = 512,\n",
    "        per_device_eval_batch_size = 512,\n",
    "\n",
    "        # control learning rate and weight decay value as a external variables. \n",
    "        learning_rate = args.lr,\n",
    "        weight_decay = args.decay,\n",
    "        remove_unused_columns = False\n",
    "    )\n",
    "\n",
    "    # save traningarguments\n",
    "    with open(od+ os.sep + \"trainingargs.json\", \"w\") as f: \n",
    "        f.write(json.dumps(trainingarguments.to_dict(), indent = 2, ensure_ascii = False))\n",
    "    f.close()\n",
    "    \n",
    "    # set trainer with autoregressive tasks\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = trainingarguments,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = training_data,\n",
    "        eval_dataset = valid_data,\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        compute_metrics = partial(metric, func = accuracy)\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "# initialize gpt model with AutoConfig\n",
    "def set_config(ggangtong_model_checkpoint, tokenizer):\n",
    "    print(len(tokenizer) - 1)\n",
    "    \n",
    "    # NOTE: head 2, layer 22, max_token_len = 1024\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        ggangtong_model_checkpoint,\n",
    "        vocab_size = len(tokenizer),\n",
    "        n_ctx = 1024,\n",
    "        bos_token_id = tokenizer.bos_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        n_embd = 64,\n",
    "        n_head = 2,\n",
    "        n_layer = 22,\n",
    "        n_positions = 1024, \n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main operations wrapper\n",
    "def main():\n",
    "\n",
    "    # get arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--lr\", type = float, required = True)\n",
    "    parser.add_argument(\"--decay\", type = float, required = True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args.lr, args.decay)\n",
    "\n",
    "    # load data\n",
    "    data_path = {\"train\": \"data_processed/training_data.csv\", \"valid\": \"data_processed/valid_data.csv\", \"test\": \"data_processed/test_data.csv\"}\n",
    "    dataset = load_dataset(\"csv\", data_files = data_path)\n",
    "\n",
    "    # get 깡통 checkpoint for config initialization\n",
    "    ggangtong_model_checkpoint = \"openai-community/gpt2\"    \n",
    "\n",
    "    # load indiv_tokenizer\n",
    "    indiv_indeces_checkpoint = \"tokenizers/tokenizer_indiv_jaeyoon\"\n",
    "    output_dir_indiv = \"weights/model_indiv\"\n",
    "    tokenizer_indiv = AutoTokenizer.from_pretrained(indiv_indeces_checkpoint)\n",
    "    tokenizer_indiv.add_special_tokens({\"pad_token\": \"<pad>\"}) # Llama3 doesn't have pad_token\n",
    "    # initialize model config\n",
    "    model_config_indiv = set_config(ggangtong_model_checkpoint, tokenizer_indiv)\n",
    "    # initialize model \n",
    "    model_indiv = GPT2LMHeadModel(model_config_indiv)\n",
    "    # initialze trainer\n",
    "    trainer_indiv = set_trainer(model_indiv, tokenizer_indiv, dataset, output_dir_indiv, args)\n",
    "    # run\n",
    "    trainer_indiv.train()\n",
    "\n",
    "\n",
    "    joint_indeces_checkpoint = \"tokenizers/tokenizer_joint_jaeyoon\"    \n",
    "    output_dir_joint = \"weights/model_joint\"    \n",
    "    tokenizer_joint = AutoTokenizer.from_pretrained(joint_indeces_checkpoint)\n",
    "    tokenizer_joint.add_special_tokens({\"pad_token\": \"<pad>\"}) # Llama3 doesn't have pad_token\n",
    "    # initialize model config\n",
    "    model_config_joint = set_config(ggangtong_model_checkpoint, tokenizer_joint)\n",
    "    # initialize model \n",
    "    model_joint = GPT2LMHeadModel(model_config_joint)\n",
    "    # initialze trainer\n",
    "    trainer_joint = set_trainer(model_joint, tokenizer_joint, dataset, output_dir_joint, args)\n",
    "    # run\n",
    "    trainer_joint.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
