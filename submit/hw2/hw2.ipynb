{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# readme.ipynb\n",
    "# proejct elements(all included in hw2.ipynb)\n",
    "- readme.ipynb: reports about methods, goal, results\n",
    "- preprocess.ipynb: preprocessing, initialize tokenizer, prepare dataset\n",
    "- train_model.ipynb: initialize model, trainingArguments and trainer, train and save model\n",
    "- run.sh: execution codes, and its result(best steps in terms of accuracy) as annotation\n",
    "\n",
    "# Methods\n",
    "- convert stock price data with korean string\n",
    "- to initialize model and train. \n",
    "- variable hyperparameters: learning rate, weight decay\n",
    "- fixed hyperparameters: layers, attention heads, sequence length\n",
    "- two controls: tokenizers\n",
    "    - first(indiv): digits and units are separatedly tokenized(예: 일/천/오/백)\n",
    "    - second(joint): digits and units are jointedly tokenized(예: 일천/오백)\n",
    "\n",
    "# Goal(what to see?)\n",
    "## As a homework\n",
    "1. preprocess the stock data\n",
    "    - convert the number into string\n",
    "    - preprocess the string fit to the tokenizer\n",
    "2. initialize GPT2 model from huggingface, using config with custom hyperparameters\n",
    "3. train and compare results\n",
    "\n",
    "## as a toy project\n",
    "1. the price data fluctuate more on low units then on the higher units\n",
    "2. according to this inductive bias, when the tokens are splitted jointedly, only the digits + lower_units varies much\n",
    "3. on the other hand, when the tokens are splitted individually, as the lower level units vary much, its digits as well as lower units would varies much, which makes model to predict the subsequent tokens(values) much harder\n",
    "4. so, I expect the accuracies of 'indiv' tokenzer would be more lower than that of 'joint' tokenizer. \n",
    "\n",
    "\n",
    "# result: \n",
    "- as we can check the evaluation result in annotation, except for the large learning rate which both model scores low, the individual tokenizer model performs much better than the joint tokenizer, which the hypothesis is rejected(note that this is without verification of statisticallity). \n",
    "- i did not decode the generated output and check how much it differs(e.g. Gen: 일십만오천이백, True: 일십만오천삼백사십, Diff: 140) so the results take more chance to be explained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run.sh\n",
    "- I run the code with shell scripts, which includes the following lines\n",
    "- this is due to the parameter controls\n",
    "- the first annotation is the accuracy of individual tokenizer, while the second is that of joint tokenizer\n",
    "\n",
    "python3 train_model.py --lr 1e-6 --decay 0.0001 # 0.7097, 0.4603\n",
    "\n",
    "python3 train_model.py --lr 1e-6 --decay 0.001 # 0.7097, 0.4603\n",
    "\n",
    "python3 train_model.py --lr 1e-6 --decay 0.01 # 0.7097, 0.4603\n",
    "\n",
    "python3 train_model.py --lr 1e-5 --decay 0.0001 # 0.2189 0.2169\n",
    "\n",
    "python3 train_model.py --lr 1e-5 --decay 0.001 # 0.2189, 0.2169\n",
    "\n",
    "python3 train_model.py --lr 1e-5 --decay 0.01 # 0.2189, 0.2169\n",
    "\n",
    "python3 train_model.py --lr 1e-4 --decay 0.0001 # 7.2611e-06, 0.0219\n",
    "\n",
    "python3 train_model.py --lr 1e-4 --decay 0.001 # 7.2611e-06, 0.0219\n",
    "\n",
    "python3 train_model.py --lr 1e-4 --decay 0.01 # 7.2611e-06, 0.0219\n",
    "\n",
    "python3 train_model.py --lr 1e-3 --decay 0.0001 # 2.4203e-07, 8.9623e-05\n",
    "\n",
    "python3 train_model.py --lr 1e-3 --decay 0.001 # 9.6815e-07, 4.1564e-05\n",
    "\n",
    "python3 train_model.py --lr 1e-3 --decay 0.01 # 7.9872e-06, 1.8184e-05?\n",
    "\n",
    "python3 train_model.py --lr 1e-7 --decay 0.0001 # 0.6116, 0.5519\n",
    "\n",
    "python3 train_model.py --lr 1e-7 --decay 0.001 # 0.6116, 0.5519\n",
    "\n",
    "python3 train_model.py --lr 1e-7 --decay 0.01 # 0.6116, 0.5519\n",
    "\n",
    "python3 train_model.py --lr 6e-7 --decay 0.0001 # 0.7329, 0.4923\n",
    "\n",
    "python3 train_model.py --lr 6e-7 --decay 0.001 # 0.7329, 0.4923\n",
    "\n",
    "python3 train_model.py --lr 6e-7 --decay 0.01 # 0.7329, 0.4923\n",
    "\n",
    "python3 train_model.py --lr 3e-7 --decay 0.0001 # 0.6276, 0.5177\n",
    "\n",
    "python3 train_model.py --lr 3e-7 --decay 0.001 # 0.6276, 0.5177\n",
    "\n",
    "python3 train_model.py --lr 3e-7 --decay 0.01 # 0.6276, 0.5177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.ipynb\n",
    "\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import os, shutil, random\n",
    "from KoreanNumber import num2kr\n",
    "import torch, torch.nn as nn\n",
    "from transformers import AutoModel, DataCollatorForLanguageModeling, AutoTokenizer\n",
    "import json, re\n",
    "\n",
    "# assume the price data are crawled\n",
    "# data format: ['date', 'time', 'data1' , 'data2', ...]\n",
    "# we will use only the first three columns \n",
    "# 5분 간격 주가 데이터\n",
    "data_raw = str(Path().resolve()) + os.sep + \"data\"\n",
    "data_processed = str(Path().resolve()) + os.sep + \"data_processed\"\n",
    "\n",
    "# gather all data into ['date' 'time' 'data1' 'data2' ... ] dataframe\n",
    "def agg_data(full_cols, target_cols, merge_cols):\n",
    "\n",
    "    files = os.listdir(data_raw)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    meta = []\n",
    "    for file in files:\n",
    "        business_name = file.split(\"_\")[0]\n",
    "        full_cols_file = [i.format(business_name) for i in full_cols]\n",
    "        target_cols_file = [i.format(business_name) for i in target_cols]\n",
    "\n",
    "        with open(f\"{data_raw}/{file}\", \"r\") as f:\n",
    "\n",
    "            x = pd.DataFrame([i.split(\"\\t\") for i in f.readlines()])\n",
    "            x = x.loc[x.index[::-1]].reset_index(drop=True)\n",
    "\n",
    "            x.columns = full_cols_file\n",
    "\n",
    "            if len(df) == 0:\n",
    "                df = x[target_cols_file]\n",
    "            else:\n",
    "                df = df.merge(x[target_cols_file], on=merge_cols)\n",
    "\n",
    "        meta.append(file)\n",
    "\n",
    "    return df, meta\n",
    "\n",
    "\n",
    "# use num2kr library, to convert number to korean(e.g. 152307 -> 일십만오만이천삼백칠)\n",
    "# for the ease of processing, use some weird expressions(e.g. 일십만오만)\n",
    "def num_to_str(out_file_name, meta_file_name, value_cols, merge_cols):\n",
    "    df = pd.read_csv(out_file_name)\n",
    "    with open(meta_file_name, \"r\") as f:\n",
    "        meta = [i.strip(\"\\n\").split(\"_\")[0] for i in f.readlines()]\n",
    "\n",
    "    meta = [i.split(\"_\")[0] for i in meta]\n",
    "    # end_{} 하나만 있으니 일단 [0]\n",
    "    value_cols_file = [value_cols[0].format(i) for i in meta]\n",
    "    x = df[value_cols_file].map(lambda x: int(str(x).replace(\",\", \"\")))\n",
    "    x = x.map(lambda x: num2kr.num2kr(x, 1))\n",
    "\n",
    "    # TODO: 누더기 코드 수정 \n",
    "    x = x.map(lambda x: re.sub(r\"십([^만])\", r\"십만\\1\", x))\n",
    "    x = pd.concat([df[merge_cols], x], axis = 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "# # make tokenizer: gpt2 tokenizer를 그대로 따와서 바꾸는 코드를 구현하려고 했지만 실패해서 손으로 직접 만듦. 추후 보완 예정.\n",
    "# def make_custom_tokenizer(old_path, new_path, new_vocab):\n",
    "#     try: os.mkdir(new_path)\n",
    "#     except: pass\n",
    "\n",
    "#     # tokenizer_config.json\n",
    "#     gpt_special_token_idx = \"50256\"\n",
    "#     special_tokens = dic['added_tokens_decoder'][gpt_special_token_idx]['content']\n",
    "#     print(special_tokens)\n",
    "\n",
    "#     with open(f\"{old_path}/tokenizer_config.json\", \"r\") as f:\n",
    "#         dic = json.load(f)\n",
    "#     f.close()\n",
    "#     key = gpt_special_token_idx\n",
    "#     val = dic['added_tokens_decoder'][gpt_special_token_idx]\n",
    "#     dic['added_tokens_decoder'] = {len(new_vocab): val}\n",
    "\n",
    "#     with open(f\"{new_path}/tokenizer_config.json\", \"w\") as f:\n",
    "#         f.write(json.dumps(dic, indent = 2, ensure_ascii = False))\n",
    "#     f.close()\n",
    "    \n",
    "#     # special_tokens_map.json\n",
    "#     shutil.copy(f\"{old_path}/special_tokens_map.json\", f\"{new_path}/special_tokens_map.json\")\n",
    "\n",
    "#     # tokenizer.json\n",
    "#     with open(f\"{old_path}/tokenizer.json\", \"r\") as f:\n",
    "#         dic = json.load(f)\n",
    "#     f.close()\n",
    "\n",
    "#     dic['added_tokens'] = [\n",
    "#         {\n",
    "#             key: (len(new_vocab) if key == \"id\" else value) \\\n",
    "#                 for key, value in dic['added_tokens'][0].items()\n",
    "#         }\n",
    "#     ] # only one token이니까 일단 0\n",
    "\n",
    "#     new_vocab_eos_added = new_vocab[len(new_vocab)] = special_tokens\n",
    "\n",
    "#     dic['model']['vocab'] = new_vocab_eos_added\n",
    "#     with open(f\"{new_path}/tokenizer.json\", \"w\") as f:\n",
    "#         f.write(json.dumps(dic, indent = 2, ensure_ascii = False))\n",
    "#     f.close()\n",
    "\n",
    "#     # vocabs\n",
    "#     with open(f\"{new_path}/vocab.json\", \"w\") as f:\n",
    "#         f.write(json.dumps(new_vocab_eos_added, indent = 2, ensure_ascii = False))\n",
    "#     f.close()\n",
    "\n",
    "#     # merge rule\n",
    "#     shutil.copy(f\"{old_path}/merges.txt\", f\"{new_path}/merges.txt\")\n",
    "    \n",
    "\n",
    "#     return \n",
    "\n",
    "# dataframe to sentences. 한 줄에 78개(=1일치 데이터)\n",
    "def convert_raw_data_to_training_data(str_file_name, training_file_name, valid_file_name, test_file_name, meta_file_name, value_cols):\n",
    "    \n",
    "    with open(meta_file_name, \"r\") as f:\n",
    "        meta = [i.strip(\"\\n\").split(\"_\")[0] for i in f.readlines()]\n",
    "\n",
    "    meta = [i.split(\"_\")[0] for i in meta]\n",
    "    # end_{} 하나만 있으니 일단 [0]\n",
    "    value_cols_file = [value_cols[0].format(i) for i in meta]\n",
    "\n",
    "    df = pd.read_csv(str_file_name)\n",
    "    lst2d = []\n",
    "    lst2d_test = []\n",
    "\n",
    "    for i in range(len(df) - 80):\n",
    "        ranges = range(i, i + 80)\n",
    "        out = df[value_cols_file].loc[ranges].apply(lambda x: \" \".join(x), axis = 0).tolist()\n",
    "        lst2d.append(out)\n",
    "    \n",
    "    for i in range(len(df)-80, len(df)):\n",
    "        ranges = range(i, min(i + 80, len(df)))\n",
    "        out = df[value_cols_file].loc[ranges].apply(lambda x: \" \".join(x), axis = 0).tolist()\n",
    "        lst2d_test.append(out)\n",
    "\n",
    "    print(len(lst2d))\n",
    "    print(len(lst2d_test))\n",
    "\n",
    "    lst1d = [i for j in lst2d for i in j]\n",
    "    lst1d_test = [i for j in lst2d_test for i in j]\n",
    "    \n",
    "    df = pd.Series(lst1d).sample(frac = 1)\n",
    "    df_test = pd.Series(lst1d_test).sample(frac = 1)\n",
    "\n",
    "    train = df.iloc[:round(len(df) * 0.8)]\n",
    "    valid = df.iloc[round(len(df) * 0.8):round(len(df) * 0.9)]\n",
    "    test = df.iloc[round(len(df) * 0.9):]\n",
    "\n",
    "\n",
    "    train.to_csv(training_file_name, index = False)\n",
    "    valid.to_csv(valid_file_name, index = False)\n",
    "    test.to_csv(test_file_name, index = False)\n",
    "\n",
    "\n",
    "    # with open(training_file_name+\"test\", \"w\") as f:\n",
    "    #     for i in lst1d_test:\n",
    "    #         f.write(f\"{i}\\n\")\n",
    "    # f.close()\n",
    "\n",
    "\n",
    "\n",
    "def main_preprocess():\n",
    "\n",
    "    full_cols = [\"date\", \"time\", \"init_{}\", \"high_{}\", \"low_{}\", \"end_{}\"] + [\n",
    "        str(i) for i in list(range(12))\n",
    "    ]\n",
    "    target_cols = [\"date\", \"time\", \"end_{}\"]\n",
    "    merge_cols = [\"date\", \"time\"]\n",
    "    value_cols = [\"end_{}\"]\n",
    "    out_file_name = f\"{data_processed}/data_aggregated.csv\"\n",
    "    meta_file_name = f\"{data_processed}/meta.txt\"\n",
    "    str_file_name = f\"{data_processed}/data_string_converted.csv\"\n",
    "    training_file_name = f\"{data_processed}/training_data.csv\"\n",
    "    valid_file_name = f\"{data_processed}/valid_data.csv\"\n",
    "    test_file_name = f\"{data_processed}/test_data.csv\"\n",
    "\n",
    "\n",
    "    ###### parse data and preprocess then save\n",
    "    df, meta = agg_data(full_cols, target_cols, merge_cols)\n",
    "    df.to_csv(out_file_name, index=False)\n",
    "    with open(meta_file_name, \"w\") as f:\n",
    "        for i in meta:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    ###### preprocess(convert number to string)\n",
    "    df = num_to_str(out_file_name, meta_file_name, value_cols, merge_cols)\n",
    "    df.to_csv(str_file_name, index = False)\n",
    "    \n",
    "    \n",
    "    # ###### prepare tokenizer: 코드로 구현하려 했지만 실패해서 토크나이저를 직접 만듦\n",
    "    # digit_name = [\"일\", \"이\", \"삼\", \"사\", \"오\", \"육\", \"칠\", \"팔\", \"구\"]\n",
    "    # unit = [\"십\", \"백\", \"천\", \"만\", \"십만\"]\n",
    "    # # zero_point = [\"영\", \"점\"] # num2kr이 소숫점을 지원하지 않음(int only) 그래서 나중에 추가해서 다시 해보는 걸로. \n",
    "    \n",
    "    # joint_tokens = [i+j for j in unit for i in digit_name] \n",
    "    # indiv_tokens = digit_name + unit\n",
    "    # # digit_only_tokens = digit_name + zero_point    \n",
    "    \n",
    "    # joint_indeces = {j: i for i, j in enumerate(tuple(joint_tokens))}\n",
    "    # indiv_indeces = {j: i for i, j in enumerate(tuple(indiv_tokens))}\n",
    "\n",
    "    # checkpoint = \"openai-community/gpt2\"\n",
    "    # tokenizer_checkpoint = \"tokenizers/tokenizer_checkpoint\"\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    # tokenizer.save_pretrained(tokenizer_checkpoint)\n",
    "\n",
    "    # joint_indeces_checkpoint = \"tokenizers/tokenizer_joint\"\n",
    "    # indiv_indeces_checkpoint = \"tokenizers/tokenizer_indiv\"\n",
    "    \n",
    "    # make_custom_tokenizer(tokenizer_checkpoint, joint_indeces_checkpoint, joint_indeces)\n",
    "    # make_custom_tokenizer(tokenizer_checkpoint, indiv_indeces_checkpoint, indiv_indeces)\n",
    "\n",
    "    \n",
    "    # convert data into training data format(put a single day's data(count: 78) to a list)\n",
    "    convert_raw_data_to_training_data(str_file_name, training_file_name, valid_file_name, test_file_name, meta_file_name, value_cols)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model.ipynb\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, GPT2LMHeadModel, AutoConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "import evaluate\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os, json\n",
    "from transformers import set_seed\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "# set seed from transformers\n",
    "set_seed(42) \n",
    "\n",
    "# trainer setter\n",
    "def set_trainer(model, tokenizer, dataset, output_dir, args):\n",
    "    \n",
    "    # load metrics\n",
    "    accuracy = evaluate.load('accuracy')\n",
    "\n",
    "    # set metric function\n",
    "    def metric(eval_pred, func):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis = -1) # (batch, sequence lenagh, hidden_state)\n",
    "        # if tokens are not the PAD token\n",
    "        filters = labels != -100\n",
    "\n",
    "        predictions = predictions[filters]\n",
    "        labels = labels[filters]\n",
    "\n",
    "        # 'func' will be 'accuracy' above\n",
    "        return func.compute(predictions = predictions, references = labels)\n",
    "    \n",
    "    # tokenizing, using 4 processor \n",
    "    def tokenize_func(examples):\n",
    "        return tokenizer(examples['0'], truncation=True, padding=True)\n",
    "    training_data = dataset['train'].map(tokenize_func, batched=True, num_proc = 4)\n",
    "    valid_data = dataset['valid'].map(tokenize_func, batched=True, num_proc = 4)\n",
    "    test_data = dataset['test'].map(tokenize_func, batched=True, num_proc = 4)\n",
    "\n",
    "    # remove original columns, leaving 'input_ids' 'attention_mask' '그리고 하나 뭐였지' only\n",
    "    training_data = training_data.remove_columns(['0'])\n",
    "    valid_data = valid_data.remove_columns(['0'])\n",
    "    test_data = test_data.remove_columns(['0'])\n",
    "\n",
    "    # make output directory of current time, where weights are being saved. \n",
    "    od = output_dir + os.sep + datetime.strftime(datetime.now(), \"%m-%d-%H-%M-%S\")\n",
    "    try: os.mkdir(od)\n",
    "    except: pass\n",
    "\n",
    "    # maximum batch, other hyperparameters except learning rate and weight decay are of defaults\n",
    "    trainingarguments = TrainingArguments(\n",
    "        do_train = True,    \n",
    "        output_dir = od,                         \n",
    "        evaluation_strategy = \"steps\", # necessary: change to step\n",
    "        save_strategy = \"steps\",                         \n",
    "        eval_steps = 50, # necessary: set step\n",
    "        save_steps = 50,\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = True, # necessary: EarlyStoppingCallBack하려면 True여야 함\n",
    "        metric_for_best_model = \"accuracy\",\n",
    "        greater_is_better = True, # necessary: higher metric results better performance # default = True when metric_for_best_model is set\n",
    "        num_train_epochs = 3,\n",
    "        seed = 42,\n",
    "        per_device_train_batch_size = 512,\n",
    "        per_device_eval_batch_size = 512,\n",
    "\n",
    "        # control learning rate and weight decay value as a external variables. \n",
    "        learning_rate = args.lr,\n",
    "        weight_decay = args.decay,\n",
    "        remove_unused_columns = False\n",
    "    )\n",
    "\n",
    "    # save traningarguments\n",
    "    with open(od+ os.sep + \"trainingargs.json\", \"w\") as f: \n",
    "        f.write(json.dumps(trainingarguments.to_dict(), indent = 2, ensure_ascii = False))\n",
    "    f.close()\n",
    "    \n",
    "    # set trainer with autoregressive tasks\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = trainingarguments,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = training_data,\n",
    "        eval_dataset = valid_data,\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        compute_metrics = partial(metric, func = accuracy)\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "# initialize gpt model with AutoConfig\n",
    "def set_config(ggangtong_model_checkpoint, tokenizer):\n",
    "    print(len(tokenizer) - 1)\n",
    "    \n",
    "    # NOTE: head 2, layer 22, max_token_len = 1024\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        ggangtong_model_checkpoint,\n",
    "        vocab_size = len(tokenizer),\n",
    "        n_ctx = 1024,\n",
    "        bos_token_id = tokenizer.bos_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        n_embd = 64,\n",
    "        n_head = 2,\n",
    "        n_layer = 22,\n",
    "        n_positions = 1024, \n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "    return config\n",
    "\n",
    "# main operations wrapper\n",
    "def main_train():\n",
    "\n",
    "    # get arguments\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"--lr\", type = float, required = True)\n",
    "    parser.add_argument(\"--decay\", type = float, required = True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(args.lr, args.decay)\n",
    "\n",
    "    # load data\n",
    "    data_path = {\"train\": \"data_processed/training_data.csv\", \"valid\": \"data_processed/valid_data.csv\", \"test\": \"data_processed/test_data.csv\"}\n",
    "    dataset = load_dataset(\"csv\", data_files = data_path)\n",
    "\n",
    "    # get 깡통 checkpoint for config initialization\n",
    "    ggangtong_model_checkpoint = \"openai-community/gpt2\"    \n",
    "\n",
    "    # load indiv_tokenizer\n",
    "    indiv_indeces_checkpoint = \"tokenizers/tokenizer_indiv_jaeyoon\"\n",
    "    output_dir_indiv = \"weights/model_indiv\"\n",
    "    tokenizer_indiv = AutoTokenizer.from_pretrained(indiv_indeces_checkpoint)\n",
    "    tokenizer_indiv.add_special_tokens({\"pad_token\": \"<pad>\"}) # Llama3 doesn't have pad_token\n",
    "    # initialize model config\n",
    "    model_config_indiv = set_config(ggangtong_model_checkpoint, tokenizer_indiv)\n",
    "    # initialize model \n",
    "    model_indiv = GPT2LMHeadModel(model_config_indiv)\n",
    "    # initialze trainer\n",
    "    trainer_indiv = set_trainer(model_indiv, tokenizer_indiv, dataset, output_dir_indiv, args)\n",
    "    # run\n",
    "    trainer_indiv.train()\n",
    "\n",
    "\n",
    "    joint_indeces_checkpoint = \"tokenizers/tokenizer_joint_jaeyoon\"    \n",
    "    output_dir_joint = \"weights/model_joint\"    \n",
    "    tokenizer_joint = AutoTokenizer.from_pretrained(joint_indeces_checkpoint)\n",
    "    tokenizer_joint.add_special_tokens({\"pad_token\": \"<pad>\"}) # Llama3 doesn't have pad_token\n",
    "    # initialize model config\n",
    "    model_config_joint = set_config(ggangtong_model_checkpoint, tokenizer_joint)\n",
    "    # initialize model \n",
    "    model_joint = GPT2LMHeadModel(model_config_joint)\n",
    "    # initialze trainer\n",
    "    trainer_joint = set_trainer(model_joint, tokenizer_joint, dataset, output_dir_joint, args)\n",
    "    # run\n",
    "    trainer_joint.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main_preprocess()\n",
    "    main_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
