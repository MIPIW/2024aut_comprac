{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import os, shutil, random\n",
    "from KoreanNumber import num2kr\n",
    "import torch, torch.nn as nn\n",
    "from transformers import AutoModel, DataCollatorForLanguageModeling, AutoTokenizer\n",
    "import json, re\n",
    "\n",
    "# assume the price data are crawled\n",
    "# data format: ['date', 'time', 'data1' , 'data2', ...]\n",
    "# we will use only the first three columns \n",
    "# 5분 간격 주가 데이터\n",
    "data_raw = str(Path().resolve()) + os.sep + \"data\"\n",
    "data_processed = str(Path().resolve()) + os.sep + \"data_processed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather all data into ['date' 'time' 'data1' 'data2' ... ] dataframe\n",
    "def agg_data(full_cols, target_cols, merge_cols):\n",
    "\n",
    "    files = os.listdir(data_raw)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    meta = []\n",
    "    for file in files:\n",
    "        business_name = file.split(\"_\")[0]\n",
    "        full_cols_file = [i.format(business_name) for i in full_cols]\n",
    "        target_cols_file = [i.format(business_name) for i in target_cols]\n",
    "\n",
    "        with open(f\"{data_raw}/{file}\", \"r\") as f:\n",
    "\n",
    "            x = pd.DataFrame([i.split(\"\\t\") for i in f.readlines()])\n",
    "            x = x.loc[x.index[::-1]].reset_index(drop=True)\n",
    "\n",
    "            x.columns = full_cols_file\n",
    "\n",
    "            if len(df) == 0:\n",
    "                df = x[target_cols_file]\n",
    "            else:\n",
    "                df = df.merge(x[target_cols_file], on=merge_cols)\n",
    "\n",
    "        meta.append(file)\n",
    "\n",
    "    return df, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use num2kr library, to convert number to korean(e.g. 152307 -> 일십만오만이천삼백칠)\n",
    "# for the ease of processing, use some weird expressions(e.g. 일십만오만)\n",
    "def num_to_str(out_file_name, meta_file_name, value_cols, merge_cols):\n",
    "    df = pd.read_csv(out_file_name)\n",
    "    with open(meta_file_name, \"r\") as f:\n",
    "        meta = [i.strip(\"\\n\").split(\"_\")[0] for i in f.readlines()]\n",
    "\n",
    "    meta = [i.split(\"_\")[0] for i in meta]\n",
    "    # end_{} 하나만 있으니 일단 [0]\n",
    "    value_cols_file = [value_cols[0].format(i) for i in meta]\n",
    "    x = df[value_cols_file].map(lambda x: int(str(x).replace(\",\", \"\")))\n",
    "    x = x.map(lambda x: num2kr.num2kr(x, 1))\n",
    "\n",
    "    # TODO: 누더기 코드 수정 \n",
    "    x = x.map(lambda x: re.sub(r\"십([^만])\", r\"십만\\1\", x))\n",
    "    x = pd.concat([df[merge_cols], x], axis = 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "# # make tokenizer: gpt2 tokenizer를 그대로 따와서 바꾸는 코드를 구현하려고 했지만 실패해서 손으로 직접 만듦. 추후 보완 예정.\n",
    "# def make_custom_tokenizer(old_path, new_path, new_vocab):\n",
    "#     try: os.mkdir(new_path)\n",
    "#     except: pass\n",
    "\n",
    "#     # tokenizer_config.json\n",
    "#     gpt_special_token_idx = \"50256\"\n",
    "#     special_tokens = dic['added_tokens_decoder'][gpt_special_token_idx]['content']\n",
    "#     print(special_tokens)\n",
    "\n",
    "#     with open(f\"{old_path}/tokenizer_config.json\", \"r\") as f:\n",
    "#         dic = json.load(f)\n",
    "#     f.close()\n",
    "#     key = gpt_special_token_idx\n",
    "#     val = dic['added_tokens_decoder'][gpt_special_token_idx]\n",
    "#     dic['added_tokens_decoder'] = {len(new_vocab): val}\n",
    "\n",
    "#     with open(f\"{new_path}/tokenizer_config.json\", \"w\") as f:\n",
    "#         f.write(json.dumps(dic, indent = 2, ensure_ascii = False))\n",
    "#     f.close()\n",
    "    \n",
    "#     # special_tokens_map.json\n",
    "#     shutil.copy(f\"{old_path}/special_tokens_map.json\", f\"{new_path}/special_tokens_map.json\")\n",
    "\n",
    "#     # tokenizer.json\n",
    "#     with open(f\"{old_path}/tokenizer.json\", \"r\") as f:\n",
    "#         dic = json.load(f)\n",
    "#     f.close()\n",
    "\n",
    "#     dic['added_tokens'] = [\n",
    "#         {\n",
    "#             key: (len(new_vocab) if key == \"id\" else value) \\\n",
    "#                 for key, value in dic['added_tokens'][0].items()\n",
    "#         }\n",
    "#     ] # only one token이니까 일단 0\n",
    "\n",
    "#     new_vocab_eos_added = new_vocab[len(new_vocab)] = special_tokens\n",
    "\n",
    "#     dic['model']['vocab'] = new_vocab_eos_added\n",
    "#     with open(f\"{new_path}/tokenizer.json\", \"w\") as f:\n",
    "#         f.write(json.dumps(dic, indent = 2, ensure_ascii = False))\n",
    "#     f.close()\n",
    "\n",
    "#     # vocabs\n",
    "#     with open(f\"{new_path}/vocab.json\", \"w\") as f:\n",
    "#         f.write(json.dumps(new_vocab_eos_added, indent = 2, ensure_ascii = False))\n",
    "#     f.close()\n",
    "\n",
    "#     # merge rule\n",
    "#     shutil.copy(f\"{old_path}/merges.txt\", f\"{new_path}/merges.txt\")\n",
    "    \n",
    "\n",
    "#     return \n",
    "\n",
    "# dataframe to sentences. 한 줄에 78개(=1일치 데이터)\n",
    "def convert_raw_data_to_training_data(str_file_name, training_file_name, valid_file_name, test_file_name, meta_file_name, value_cols):\n",
    "    \n",
    "    with open(meta_file_name, \"r\") as f:\n",
    "        meta = [i.strip(\"\\n\").split(\"_\")[0] for i in f.readlines()]\n",
    "\n",
    "    meta = [i.split(\"_\")[0] for i in meta]\n",
    "    # end_{} 하나만 있으니 일단 [0]\n",
    "    value_cols_file = [value_cols[0].format(i) for i in meta]\n",
    "\n",
    "    df = pd.read_csv(str_file_name)\n",
    "    lst2d = []\n",
    "    lst2d_test = []\n",
    "\n",
    "    for i in range(len(df) - 80):\n",
    "        ranges = range(i, i + 80)\n",
    "        out = df[value_cols_file].loc[ranges].apply(lambda x: \" \".join(x), axis = 0).tolist()\n",
    "        lst2d.append(out)\n",
    "    \n",
    "    for i in range(len(df)-80, len(df)):\n",
    "        ranges = range(i, min(i + 80, len(df)))\n",
    "        out = df[value_cols_file].loc[ranges].apply(lambda x: \" \".join(x), axis = 0).tolist()\n",
    "        lst2d_test.append(out)\n",
    "\n",
    "    print(len(lst2d))\n",
    "    print(len(lst2d_test))\n",
    "\n",
    "    lst1d = [i for j in lst2d for i in j]\n",
    "    lst1d_test = [i for j in lst2d_test for i in j]\n",
    "    \n",
    "    df = pd.Series(lst1d).sample(frac = 1)\n",
    "    df_test = pd.Series(lst1d_test).sample(frac = 1)\n",
    "\n",
    "    train = df.iloc[:round(len(df) * 0.8)]\n",
    "    valid = df.iloc[round(len(df) * 0.8):round(len(df) * 0.9)]\n",
    "    test = df.iloc[round(len(df) * 0.9):]\n",
    "\n",
    "\n",
    "    train.to_csv(training_file_name, index = False)\n",
    "    valid.to_csv(valid_file_name, index = False)\n",
    "    test.to_csv(test_file_name, index = False)\n",
    "\n",
    "\n",
    "    # with open(training_file_name+\"test\", \"w\") as f:\n",
    "    #     for i in lst1d_test:\n",
    "    #         f.write(f\"{i}\\n\")\n",
    "    # f.close()\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    full_cols = [\"date\", \"time\", \"init_{}\", \"high_{}\", \"low_{}\", \"end_{}\"] + [\n",
    "        str(i) for i in list(range(12))\n",
    "    ]\n",
    "    target_cols = [\"date\", \"time\", \"end_{}\"]\n",
    "    merge_cols = [\"date\", \"time\"]\n",
    "    value_cols = [\"end_{}\"]\n",
    "    out_file_name = f\"{data_processed}/data_aggregated.csv\"\n",
    "    meta_file_name = f\"{data_processed}/meta.txt\"\n",
    "    str_file_name = f\"{data_processed}/data_string_converted.csv\"\n",
    "    training_file_name = f\"{data_processed}/training_data.csv\"\n",
    "    valid_file_name = f\"{data_processed}/valid_data.csv\"\n",
    "    test_file_name = f\"{data_processed}/test_data.csv\"\n",
    "\n",
    "\n",
    "    ###### parse data and preprocess then save\n",
    "    df, meta = agg_data(full_cols, target_cols, merge_cols)\n",
    "    df.to_csv(out_file_name, index=False)\n",
    "    with open(meta_file_name, \"w\") as f:\n",
    "        for i in meta:\n",
    "            f.write(str(i) + \"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "    ###### preprocess(convert number to string)\n",
    "    df = num_to_str(out_file_name, meta_file_name, value_cols, merge_cols)\n",
    "    df.to_csv(str_file_name, index = False)\n",
    "    \n",
    "    \n",
    "    # ###### prepare tokenizer: 코드로 구현하려 했지만 실패해서 토크나이저를 직접 만듦\n",
    "    # digit_name = [\"일\", \"이\", \"삼\", \"사\", \"오\", \"육\", \"칠\", \"팔\", \"구\"]\n",
    "    # unit = [\"십\", \"백\", \"천\", \"만\", \"십만\"]\n",
    "    # # zero_point = [\"영\", \"점\"] # num2kr이 소숫점을 지원하지 않음(int only) 그래서 나중에 추가해서 다시 해보는 걸로. \n",
    "    \n",
    "    # joint_tokens = [i+j for j in unit for i in digit_name] \n",
    "    # indiv_tokens = digit_name + unit\n",
    "    # # digit_only_tokens = digit_name + zero_point    \n",
    "    \n",
    "    # joint_indeces = {j: i for i, j in enumerate(tuple(joint_tokens))}\n",
    "    # indiv_indeces = {j: i for i, j in enumerate(tuple(indiv_tokens))}\n",
    "\n",
    "    # checkpoint = \"openai-community/gpt2\"\n",
    "    # tokenizer_checkpoint = \"tokenizers/tokenizer_checkpoint\"\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    # tokenizer.save_pretrained(tokenizer_checkpoint)\n",
    "\n",
    "    # joint_indeces_checkpoint = \"tokenizers/tokenizer_joint\"\n",
    "    # indiv_indeces_checkpoint = \"tokenizers/tokenizer_indiv\"\n",
    "    \n",
    "    # make_custom_tokenizer(tokenizer_checkpoint, joint_indeces_checkpoint, joint_indeces)\n",
    "    # make_custom_tokenizer(tokenizer_checkpoint, indiv_indeces_checkpoint, indiv_indeces)\n",
    "\n",
    "    \n",
    "    # convert data into training data format(put a single day's data(count: 78) to a list)\n",
    "    convert_raw_data_to_training_data(str_file_name, training_file_name, valid_file_name, test_file_name, meta_file_name, value_cols)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runner\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
