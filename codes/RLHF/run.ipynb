{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from openpyxl import load_workbook\n",
    "import pandas as pd, numpy as np\n",
    "from argparse import Namespace\n",
    "from datetime import datetime\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re, json\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, GPT2LMHeadModel\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForSequenceClassification, TrainingArguments\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, RewardTrainer, RewardConfig, get_peft_config, SFTConfig\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "# Add the ~/myUtil directory to sys.path\n",
    "sys.path.append(os.path.expanduser('~/'))\n",
    "from myUtils.timeUtils import TimeUtils\n",
    "from myUtils.IOUtils import IOUtils\n",
    "from KoreanNumber import num2kr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPPOTrainer(PPOTrainer):\n",
    "    def prepare_dataloader(self, dataset, data_collator=None):\n",
    "        \"\"\"\n",
    "        Prepare the dataloader for training.\n",
    "\n",
    "        Args:\n",
    "            dataset (Union[`torch.utils.data.Dataset`, `datasets.Dataset`]):\n",
    "                PyTorch dataset or Hugging Face dataset. If a Hugging Face dataset is passed, the dataset\n",
    "                will be preprocessed by removing the columns that are not used by the model.\n",
    "            data_collator (Optional[function]):\n",
    "                Data collator function.\n",
    "\n",
    "        Returns:\n",
    "            `torch.utils.data.DataLoader`: PyTorch dataloader\n",
    "        \"\"\"\n",
    "        # if isinstance(dataset, Dataset):\n",
    "        #     dataset = self._remove_unused_columns(dataset)\n",
    "        print(\"----------------\", dataset)\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            collate_fn=data_collator,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "        return dataloader\n",
    "        \n",
    "def _num_to_str(data):\n",
    "    if isinstance(data, str):\n",
    "        data = [data]\n",
    "\n",
    "    data = [num2kr.num2kr(i, 1) for i in data]    \n",
    "    \n",
    "    return [re.sub(r\"Ïã≠([^Îßå])\", r\"Ïã≠Îßå\\1\", i) for i in data]\n",
    "\n",
    "\n",
    "def preprocess(args):\n",
    "\n",
    "    if os.path.isfile(args.name_processed_files):\n",
    "        with open(args.name_processed_files, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            return data\n",
    "        \n",
    "    files = os.listdir(args.dir_files)\n",
    "    \n",
    "    data = []\n",
    "    for file in files:\n",
    "       \n",
    "        load_wb = load_workbook(args.dir_files + os.sep + file, data_only=True)    \n",
    "        load_ws = load_wb[load_wb.sheetnames[0]]\n",
    "\n",
    "        # Extract data in a table-like format (list of dictionaries)\n",
    "        header = [header for header in load_ws.iter_rows(min_row=1, max_row = 1, values_only=True)][0]\n",
    "        rows = [row for row in load_ws.iter_rows(min_row=2, values_only=True)]\n",
    "\n",
    "        x = pd.DataFrame(rows)\n",
    "        x.columns = header\n",
    "        x = x[args.columns_need]\n",
    "        x['ÏùºÏûê'] = x['ÏùºÏûê'].map(lambda x : datetime.strptime(x, \"%Y/%m/%d\") if pd.notna(x) and x != '' else None)\n",
    "        x = x.dropna(subset = ['ÏùºÏûê'])\n",
    "        # List(ÏùºÏûê)[List(Í∞ÄÍ≤©)[int]]\n",
    "        # List(carrier[List(Í∞ÄÍ≤©)[int]])\n",
    "        data.append(x.groupby(\"ÏùºÏûê\")['Ï¢ÖÍ∞Ä'].apply(list).tolist())\n",
    "    \n",
    "    data = [i for j in data for i in j]\n",
    "    data = [i[:int(len(i)/2)] for i in data] + [i[int(len(i)/2):] for i in data]\n",
    "    with open(args.name_processed_files, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_distribution(data):\n",
    "    d = [[abs(i2 - i1) for i1, i2 in zip(item, item[1:])] for item in data]\n",
    "    d = [i for j in d for i in j]\n",
    "    return round(sum(d) / len(d), 2)\n",
    "\n",
    "\n",
    "# pretrain/preproces.pyÎ°ú  preprocessÎêú Îç∞Ïù¥ÌÑ∞Î•º Ìïú Î≤à Îçî preprocessÌïòÎäî Î™®Îìà\n",
    "def _str_to_num(list_x):\n",
    "    digits = {'Ïùº': 1, 'Ïù¥': 2, 'ÏÇº': 3, 'ÏÇ¨': 4, 'Ïò§': 5, 'Ïú°': 6, 'Ïπ†': 7, 'Ìåî': 8, 'Íµ¨': 9}\n",
    "    units = {'Î®ï': 100000, 'Îßå': 10000, 'Ï≤ú': 1000, 'Î∞±': 100, 'Ïã≠': 10} # Î®ï: 10Îßå\n",
    "    if isinstance(list_x, str):\n",
    "        list_x = [list_x]\n",
    "\n",
    "    list_out = []\n",
    "    for i in list_x:        \n",
    "        # Ï§ëÎ≥µÎêú Ïà´Ïûê Î∞è Îã®ÏúÑ Ï≤òÎ¶¨\n",
    "        korean_str = re.sub(r'Ïã≠Îßå', \"Î®ï\", i)  # Ïà´Ïûê Ï§ëÎ≥µ Ï†úÍ±∞\n",
    "\n",
    "        units_only = re.sub(r\"[ÏùºÏù¥ÏÇºÏÇ¨Ïò§Ïú°Ïπ†ÌåîÍµ¨]\", \"_\", korean_str).split(\"_\")            \n",
    "        units_only = [units[j[0]] for j in units_only if j != \"\"]\n",
    "\n",
    "        digits_only = re.sub(r\"[^ÏùºÏù¥ÏÇºÏÇ¨Ïò§Ïú°Ïπ†ÌåîÍµ¨]\", \"_\", korean_str).split(\"_\")\n",
    "        digits_only = [digits[j[0]] for j in digits_only if j != \"\"]\n",
    "        \n",
    "        out = 0\n",
    "        for i, j in zip(units_only, digits_only):\n",
    "            out += i * j\n",
    "        \n",
    "        list_out.append(out)\n",
    "    \n",
    "    return list_out\n",
    "\n",
    "def set_reward_dataset(data):\n",
    "\n",
    "    if isinstance(data, int):\n",
    "        data = [data]\n",
    "\n",
    "    rejected_data = []\n",
    "    # Ïã≠ÎßåÏùò ÏûêÎ¶¨Ïàò Î∞è ÎßåÏùò ÏûêÎ¶¨ÏàòÏóêÏÑú Î∞îÎÄåÎäî Îç∞Ïù¥ÌÑ∞Îäî Ï†ÅÏ†àÌïòÏßÄ ÏïäÎã§. \n",
    "    for example in data:\n",
    "        rejected_exmample = []\n",
    "        for dataIdx in range(len(example)):\n",
    "            x = int(round(random.randint(10000, 100000), -4))\n",
    "            rejected_exmample.append(min(example[dataIdx] + x, 200000))\n",
    "        rejected_data.append(rejected_exmample)\n",
    "\n",
    "    rejected_data = [\" \".join(_num_to_str(i)) for i in rejected_data]\n",
    "    data = [\" \".join(_num_to_str(i)) for i in data]\n",
    "    dataset = Dataset.from_dict({\"chosen\": data, \"rejected\": rejected_data})\n",
    "    return dataset\n",
    "\n",
    "def set_ppo_dataset(data):\n",
    "\n",
    "    data = [\" \".join(_num_to_str(i)) for i in data]\n",
    "    return Dataset.from_dict({\"query\": data})\n",
    "\n",
    "def set_normal_dataset(data):\n",
    "    data = [\" \".join(_num_to_str(i)) for i in data]\n",
    "    return Dataset.from_dict({\"text\": data})\n",
    "\n",
    "    return reward_trainer\n",
    "\n",
    "\n",
    "def set_reward_model(args):\n",
    "    #Select a base model whch we need to train for reward modeling.\n",
    "    model_name = args.reward_checkpoint\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    reward_model.config.pad_token_id = reward_model.config.eos_token_id\n",
    "\n",
    "    return reward_model, tokenizer\n",
    "\n",
    "def set_ppo_model(args):\n",
    "    config = PPOConfig(\n",
    "        model_name=args.checkpoint,\n",
    "        learning_rate=1e-7,\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer, config\n",
    "\n",
    "def set_normal_model(args):\n",
    "\n",
    "    model = GPT2LMHeadModel.from_pretrained(args.checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_joint)    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def set_reward_model_trainer(args, dataset_train, dataset_eval, model, tokenizer):\n",
    "\n",
    "    def formatting_func(examples):\n",
    "        kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
    "        tokens_chosen = tokenizer.batch_encode_plus(examples[\"chosen\"], **kwargs)\n",
    "        tokens_rejected = tokenizer.batch_encode_plus(examples[\"rejected\"], **kwargs)\n",
    "        return {\n",
    "            \"input_ids_chosen\": tokens_chosen[\"input_ids\"], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"],\n",
    "            \"input_ids_rejected\": tokens_rejected[\"input_ids\"], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"]\n",
    "        }\n",
    "\n",
    "    formatted_dataset = {}\n",
    "    formatted_dataset['train'] = dataset_train.map(formatting_func, batched = True, num_proc = args.num_cores)\n",
    "    formatted_dataset['test'] = dataset_eval.map(formatting_func, batched = True, num_proc = args.num_cores)\n",
    "\n",
    "    # Configuring the training arguments\n",
    "    training_args = RewardConfig(\n",
    "        output_dir=args.output_reward_checkpoint,\n",
    "        per_device_train_batch_size=16,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_steps=1,\n",
    "        num_train_epochs = 3,\n",
    "        report_to=None,\n",
    "        # center_rewards_coefficient=0.01,\n",
    "    )\n",
    "\n",
    "    reward_trainer = RewardTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=formatted_dataset['train'],\n",
    "        eval_dataset=formatted_dataset['test'],\n",
    "\n",
    "    )\n",
    "\n",
    "    return reward_trainer\n",
    "\n",
    "\n",
    "def set_ppo_trainer(args, dataset, model, tokenizer, config):\n",
    "\n",
    "    def tokenize(sample):\n",
    "        kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"], **kwargs)\n",
    "        return sample\n",
    "\n",
    "    dataset = dataset.map(tokenize, batched=False)\n",
    "\n",
    "    ppo_trainer = MyPPOTrainer(\n",
    "        model=model,\n",
    "        config=config,\n",
    "        dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    return ppo_trainer, dataset\n",
    "\n",
    "\n",
    "def set_normal_trainer(args, dataset_train, dataset_valid, model, tokenizer):\n",
    "        \n",
    "    accuracy = evaluate.load('accuracy')\n",
    "    def metric(eval_pred, func):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis = -1) # (batch, sequence lenagh, hidden_state)\n",
    "        filters = labels != -100\n",
    "\n",
    "        predictions = predictions[filters]\n",
    "        labels = labels[filters]\n",
    "        return func.compute(predictions = predictions, references = labels)\n",
    "    \n",
    "    def tokenize_func(examples):\n",
    "        kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": 512, \"return_tensors\": \"pt\"}\n",
    "        return tokenizer(examples['text'], **kwargs)\n",
    "\n",
    "    training_data = dataset_train.map(tokenize_func, batched=True, num_proc = 4)\n",
    "    valid_data = dataset_valid.map(tokenize_func, batched=True, num_proc = 4)\n",
    "\n",
    "    training_data = training_data.remove_columns(['text'])\n",
    "    valid_data = valid_data.remove_columns(['text'])\n",
    "\n",
    "    od = args.output_normal_checkpoint + os.sep + datetime.strftime(datetime.now(), \"%m-%d-%H-%M-%S\")\n",
    "    try: os.mkdir(od)\n",
    "    except: pass\n",
    "    trainingarguments = TrainingArguments(\n",
    "        do_train = True,    \n",
    "        output_dir = od,                         \n",
    "        evaluation_strategy = \"steps\", # necessary: change to step\n",
    "        save_strategy = \"steps\",                         \n",
    "        eval_steps = 50, # necessary: set step\n",
    "        save_steps = 50,\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = True, # necessary: EarlyStoppingCallBackÌïòÎ†§Î©¥ TrueÏó¨Ïïº Ìï®\n",
    "        metric_for_best_model = \"accuracy\",\n",
    "        greater_is_better = True, # necessary: higher metric results better performance # default = True when metric_for_best_model is set\n",
    "        num_train_epochs = 10,\n",
    "        seed = 42,\n",
    "        per_device_train_batch_size = 512,\n",
    "        per_device_eval_batch_size = 512,\n",
    "\n",
    "        # eval_accumulation_steps = 50,\n",
    "        learning_rate = 1e-7,\n",
    "        remove_unused_columns = False\n",
    "    )\n",
    "\n",
    "    with open(od+ os.sep + \"trainingargs.json\", \"w\") as f: \n",
    "        f.write(json.dumps(trainingarguments.to_dict(), indent = 2, ensure_ascii = False))\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = trainingarguments,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = training_data,\n",
    "        eval_dataset = valid_data,\n",
    "        data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "        compute_metrics = partial(metric, func = accuracy)\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def ppo_trainer_train(ppoTrainer, dataset_train, tokenizer, reward_model):\n",
    "    generation_kwargs = {\n",
    "        \"min_length\": -1,\n",
    "        \"top_k\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    epochs = 10\n",
    "    for _ in tqdm(range(epochs), \"epoch: \"):\n",
    "        for batch in tqdm(ppoTrainer.prepare_dataloader(dataset_train)):\n",
    "            query_tensors = batch[\"input_ids\"]\n",
    "            inputs = torch.stack(query_tensors)\n",
    "\n",
    "            #### Get response from SFTModel\n",
    "            response_tensors = ppoTrainer.generate(inputs, **generation_kwargs)\n",
    "            batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "\n",
    "            #### Compute reward score\n",
    "            texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "            pipe_outputs = reward_model(texts)\n",
    "            rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "\n",
    "            #### Run PPO step\n",
    "            stats = ppoTrainer.step(query_tensors, response_tensors, rewards)\n",
    "            ppoTrainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    #### Save model\n",
    "    ppoTrainer.save_pretrained(args.output_rlhf_checkpoint)\n",
    "\n",
    "\n",
    "def generate_decode(args, model_checkpoint, dataset_test):\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    def tokenize_func(examples):\n",
    "        return tokenizer(\" \".join(examples['text'].split(\" \")[:10]), truncation=True, padding=True)\n",
    "\n",
    "    def get_gold(examples):\n",
    "        return \" \".join(examples['text'].split(\" \")[10:])\n",
    "    \n",
    "    gold_value = dataset_test.map(get_gold, batched = True, num_proce = 4)\n",
    "    test_data = dataset_test.map(tokenize_func, batched=True, num_proc = 4)\n",
    "\n",
    "    # Generate predictions\n",
    "    decoded_outputs = []\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "        for example in test_data:\n",
    "            input_ids = torch.tensor(example['input_ids']).unsqueeze(0)  # Add batch dimension\n",
    "            outputs = model(input_ids)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            # Decode the predicted tokens to text\n",
    "            decoded_text = tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
    "            decoded_outputs.append(decoded_text)\n",
    "    \n",
    "    return decoded_outputs, gold_value\n",
    "\n",
    "def calculate_gap(args, decoded_outputs, gold_value):\n",
    "    decoded_outputs = [_str_to_num(i) for i in decoded_outputs]\n",
    "    gold_value = [_str_to_num(i) for i in gold_value]\n",
    "    \n",
    "    min_length = [min(len(i), len(j)) for i, j in zip(decoded_outputs, gold_value)]\n",
    "    decoded_outputs = [decoded_outputs[:i] for i in min_length]\n",
    "    gold_value = [gold_value[:i] for i in min_length]\n",
    "\n",
    "    return sum([sum([abs(ii-jj) for ii, jj in zip(i, j)]) for i, j in zip(decoded_outputs, gold_value)])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@TimeUtils.consumedTime_decorator # the arguments should only be a single namespace object\n",
    "def main(args):\n",
    "    \n",
    "    # random.seed(42)\n",
    "    # data = preprocess(args)\n",
    "    # random.shuffle(data)\n",
    "\n",
    "    # dist = get_distribution(data)\n",
    "    # x = _num_to_str(data[1234])\n",
    "    # decode_sample = _str_to_num(x)\n",
    "\n",
    "    # half = int(len(data)/2)\n",
    "    # dataset_reward = set_reward_dataset(data[:half])\n",
    "    # dataset_reward_train_test = dataset_reward.train_test_split(0.2)\n",
    "    # dataset_reward_train = dataset_reward_train_test['train']\n",
    "    # dataset_reward_valid = dataset_reward_train_test['test'] \n",
    "\n",
    "    # dataset_ppo = set_ppo_dataset(data[half:])\n",
    "    # dataset_ppo_train_test = dataset_ppo.train_test_split(0.2)\n",
    "    # dataset_ppo_train = dataset_ppo_train_test['train']\n",
    "    # dataset_ppo_valid_test = dataset_ppo_train_test['test'].train_test_split(0.5)\n",
    "    # dataset_ppo_valid, dataset_ppo_test = dataset_ppo_valid_test['train'], dataset_ppo_valid_test['test']\n",
    "\n",
    "    # dataset_normal = set_normal_dataset(data[half:])\n",
    "    # dataset_normal_train_test = dataset_normal.train_test_split(0.2)\n",
    "    # dataset_normal_train = dataset_normal_train_test['train']\n",
    "    # dataset_normal_valid_test = dataset_normal_train_test['test'].train_test_split(0.5)\n",
    "    # dataset_normal_valid, dataset_normal_test = dataset_normal_valid_test['train'], dataset_normal_valid_test['test']\n",
    "\n",
    "\n",
    "    # normalModel, normalTokenizer = set_normal_model(args)\n",
    "    # normalTrainer = set_normal_trainer(args, dataset_normal_train, dataset_normal_valid, normalModel, normalTokenizer)\n",
    "    # reward_model, reward_tokenizer = set_reward_model(args)\n",
    "    # rewardTrainer = set_reward_model_trainer(args, dataset_reward_train, dataset_reward_valid, reward_model, reward_tokenizer)\n",
    "    # ppoModel, ppoTokenizer, ppoConfig = set_ppo_model(args)\n",
    "    # ppoTrainer, ppoDataset = set_ppo_trainer(args, dataset_ppo_train, ppoModel, ppoTokenizer, ppoConfig)\n",
    "    \n",
    "    # # rewardTrainer.train()\n",
    "    # ppo_trainer_train(ppoTrainer, ppoDataset, ppoTokenizer, reward_model)\n",
    "    # normalTrainer.train()    \n",
    "\n",
    "    # output_rlhf, gold_rlfh = generate_decode(args, args.output_rlhf_checkpoint, dataset_normal_test)\n",
    "    # output_normal, gold_normal = generate_decode(args, args.output_normal_checkpoint, dataset_normal_test)\n",
    "\n",
    "    # print(calculate_gap(args, output_rlhf, gold_rlfh))\n",
    "    # print(calculate_gap(args, output_normal, gold_normal))\n",
    "\n",
    "    # # dataset_reward = set_reward_dataset(data[:half])\n",
    "    # # dataset_reward_train, dataset_reward_valid = dataset_reward.train_test_split(0.1)\n",
    "    # # reward_model, reward_tokenizer = set_reward_model(args)\n",
    "    # # rewardTrainer = set_reward_model_trainer(args, dataset_reward_train, dataset_reward_valid, reward_tokenizer)\n",
    "    # # rewardTrainer.train()\n",
    "\n",
    "    # # dataset_ppo = set_ppo_dataset(data[half:])\n",
    "    # # dataset_ppo_train, dataset_ppo_test = dataset_ppo.train_test_split(0.2)\n",
    "    # # dataset_ppo_valid, dataset_ppo_test = dataset_ppo_test.train_test_split(0.5)\n",
    "    # # ppoModel, ppoTokenizer, ppoConfig = set_ppo_model(args)\n",
    "    # # ppoTrainer = set_ppo_trainer(args, dataset_ppo_train, ppoModel, ppoTokenizer, ppoConfig)\n",
    "    # # ppo_trainer_train(ppoTrainer, dataset_ppo_train, ppoTokenizer, reward_model)\n",
    "\n",
    "    # # dataset_normal = set_normal_dataset(data[half:])\n",
    "    # # dataset_normal_train, dataset_normal_test = dataset_normal.train_test_split(0.2)\n",
    "    # # dataset_normal_valid, dataset_normal_test = dataset_normal_test.train_test_split(0.5)\n",
    "    # # normalModel, normalTokenizer = set_normal_model(args)\n",
    "    # # normalTrainer = set_normal_trainer(args, dataset_normal_train, dataset_normal_valid, normalModel, normalTokenizer)\n",
    "    # # normalTrainer.train()    \n",
    "\n",
    "    # # output_rlhf, gold_rlfh = generate_decode(args, args.output_rlhf_checkpoint, dataset_normal_test)\n",
    "    # # output_normal, gold_normal = generate_decode(args, args.output_normal_checkpoint, dataset_normal_test)\n",
    "\n",
    "    # # print(calculate_gap(args, output_rlhf, gold_rlfh))\n",
    "    # # print(calculate_gap(args, output_normal, gold_normal))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job starts at 2024-11-11 19:28:54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50916ac0ff1744c88716793a94bc39a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8161d9139ac54049b56f063c3805e046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/622 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab50632129c4404ebf5c53b406b83b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/4980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d17b0c4af84aab8ad7e3136b88d49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/trl/trainer/reward_trainer.py:175: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/hyohyeongjang/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/trl/trainer/reward_trainer.py:192: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90effec23f6a4b4481a4e50409495480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dataset({\n",
      "    features: ['query', 'input_ids'],\n",
      "    num_rows: 4980\n",
      "})\n",
      "result------------------------- (AutoModelForCausalLMWithValueHead(\n",
      "  (pretrained_model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(54, 128)\n",
      "      (wpe): Embedding(1024, 128)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-3): 4 x GPT2Block(\n",
      "          (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2SdpaAttention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=128, out_features=54, bias=False)\n",
      "  )\n",
      "  (v_head): ValueHead(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (summary): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "), AcceleratedOptimizer (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 1e-07\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      "), DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='/home/hyohyeongjang/2024aut_comprac/weights/model_joint/10-15-02-51-17/checkpoint-50', vocab_size=52, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'pad_token': '<eos>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t52: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t53: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt'), <accelerate.data_loader.DataLoaderShard object at 0x7f8ed3b41700>, None)\n",
      "result------------------------- (AutoModelForCausalLMWithValueHead(\n",
      "  (pretrained_model): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(54, 128)\n",
      "      (wpe): Embedding(1024, 128)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-3): 4 x GPT2Block(\n",
      "          (ln_1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2SdpaAttention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=128, out_features=54, bias=False)\n",
      "  )\n",
      "  (v_head): ValueHead(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (summary): Linear(in_features=128, out_features=1, bias=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Dataset({\n",
      "    features: ['query', 'input_ids'],\n",
      "    num_rows: 4980\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/38 [00:00<?, ?it/s]\n",
      "epoch:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m\n\u001b[1;32m     12\u001b[0m num_cores \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     14\u001b[0m args \u001b[38;5;241m=\u001b[39m Namespace(\n\u001b[1;32m     15\u001b[0m     dir_files \u001b[38;5;241m=\u001b[39m dir_files,\n\u001b[1;32m     16\u001b[0m     name_processed_files \u001b[38;5;241m=\u001b[39m name_processed_files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     num_cores \u001b[38;5;241m=\u001b[39m num_cores,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myUtils/timeUtils.py:21\u001b[0m, in \u001b[0;36mTimeUtils.consumedTime_decorator.<locals>.wrapper\u001b[0;34m(namespace)\u001b[0m\n\u001b[1;32m     18\u001b[0m d \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrftime(datetime\u001b[38;5;241m.\u001b[39mnow(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob starts at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m s1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     24\u001b[0m d \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mstrftime(datetime\u001b[38;5;241m.\u001b[39mnow(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 399\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    396\u001b[0m ppoTrainer, ppoDataset \u001b[38;5;241m=\u001b[39m set_ppo_trainer(args, dataset_ppo_train, ppoModel, ppoTokenizer, ppoConfig)\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# rewardTrainer.train()\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m \u001b[43mppo_trainer_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mppoTrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppoDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mppoTokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m normalTrainer\u001b[38;5;241m.\u001b[39mtrain()    \n\u001b[1;32m    402\u001b[0m output_rlhf, gold_rlfh \u001b[38;5;241m=\u001b[39m generate_decode(args, args\u001b[38;5;241m.\u001b[39moutput_rlhf_checkpoint, dataset_normal_test)\n",
      "Cell \u001b[0;32mIn[2], line 300\u001b[0m, in \u001b[0;36mppo_trainer_train\u001b[0;34m(ppoTrainer, dataset_train, tokenizer, reward_model)\u001b[0m\n\u001b[1;32m    298\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(ppoTrainer\u001b[38;5;241m.\u001b[39mprepare_dataloader(dataset_train)):\n\u001b[1;32m    301\u001b[0m         query_tensors \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m#### Get response from SFTModel\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:154\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m--> 154\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:154\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m--> 154\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/hyohyeongjang_base/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:169\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    167\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    170\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dir_files = \"/home/hyohyeongjang/2024aut_comprac/data/data_rlhf\"\n",
    "    name_processed_files = \"/home/hyohyeongjang/2024aut_comprac/data/data_rlhf_processed/data_rlhf.pk\"\n",
    "    columns_need = ['ÏùºÏûê', 'Ï¢ÖÍ∞Ä']\n",
    "    checkpoint = \"/home/hyohyeongjang/2024aut_comprac/weights/model_joint/10-15-02-51-17/checkpoint-50\"\n",
    "    tokenizer_joint = \"/home/hyohyeongjang/2024aut_comprac/tokenizers/tokenizer_joint_jaeyoon\"\n",
    "    reward_model_checkpoint = \"distilroberta-base\"\n",
    "    # already trained\n",
    "    output_reward_checkpoint = \"/home/hyohyeongjang/2024aut_comprac/weights/reward_model/checkpoint-936\"\n",
    "    output_rlhf_checkpoint = \"/home/hyohyeongjang/2024aut_comprac/weights/rlhf_result\"\n",
    "    output_normal_checkpoint = \"/home/hyohyeongjang/2024aut_comprac/weights/no_rlhf_result\"\n",
    "    num_cores = 4\n",
    "\n",
    "    args = Namespace(\n",
    "        dir_files = dir_files,\n",
    "        name_processed_files = name_processed_files,\n",
    "        columns_need = columns_need,\n",
    "        checkpoint = checkpoint,\n",
    "        tokenizer_joint = tokenizer_joint,\n",
    "        reward_checkpoint = reward_model_checkpoint,\n",
    "        output_reward_checkpoint = output_reward_checkpoint,\n",
    "        output_rlhf_checkpoint = output_rlhf_checkpoint,\n",
    "        output_normal_checkpoint = output_normal_checkpoint,\n",
    "        num_cores = num_cores,\n",
    "    )\n",
    "\n",
    "\n",
    "    random.seed(42)\n",
    "    data = preprocess(args)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    dist = get_distribution(data)\n",
    "    x = _num_to_str(data[1234])\n",
    "    decode_sample = _str_to_num(x)\n",
    "\n",
    "    half = int(len(data)/2)\n",
    "    dataset_reward = set_reward_dataset(data[:half])\n",
    "    dataset_reward_train_test = dataset_reward.train_test_split(0.2)\n",
    "    dataset_reward_train = dataset_reward_train_test['train']\n",
    "    dataset_reward_valid = dataset_reward_train_test['test'] \n",
    "\n",
    "    dataset_ppo = set_ppo_dataset(data[half:])\n",
    "    dataset_ppo_train_test = dataset_ppo.train_test_split(0.2)\n",
    "    dataset_ppo_train = dataset_ppo_train_test['train']\n",
    "    dataset_ppo_valid_test = dataset_ppo_train_test['test'].train_test_split(0.5)\n",
    "    dataset_ppo_valid, dataset_ppo_test = dataset_ppo_valid_test['train'], dataset_ppo_valid_test['test']\n",
    "\n",
    "    dataset_normal = set_normal_dataset(data[half:])\n",
    "    dataset_normal_train_test = dataset_normal.train_test_split(0.2)\n",
    "    dataset_normal_train = dataset_normal_train_test['train']\n",
    "    dataset_normal_valid_test = dataset_normal_train_test['test'].train_test_split(0.5)\n",
    "    dataset_normal_valid, dataset_normal_test = dataset_normal_valid_test['train'], dataset_normal_valid_test['test']\n",
    "\n",
    "\n",
    "    normalModel, normalTokenizer = set_normal_model(args)\n",
    "    normalTrainer = set_normal_trainer(args, dataset_normal_train, dataset_normal_valid, normalModel, normalTokenizer)\n",
    "    reward_model, reward_tokenizer = set_reward_model(args)\n",
    "    rewardTrainer = set_reward_model_trainer(args, dataset_reward_train, dataset_reward_valid, reward_model, reward_tokenizer)\n",
    "    ppoModel, ppoTokenizer, ppoConfig = set_ppo_model(args)\n",
    "    ppoTrainer, ppoDataset = set_ppo_trainer(args, dataset_ppo_train, ppoModel, ppoTokenizer, ppoConfig)\n",
    "    \n",
    "    # rewardTrainer.train()\n",
    "    ppo_trainer_train(ppoTrainer, ppoDataset, ppoTokenizer, reward_model)\n",
    "    normalTrainer.train()    \n",
    "\n",
    "    output_rlhf, gold_rlfh = generate_decode(args, args.output_rlhf_checkpoint, dataset_normal_test)\n",
    "    output_normal, gold_normal = generate_decode(args, args.output_normal_checkpoint, dataset_normal_test)\n",
    "\n",
    "    print(calculate_gap(args, output_rlhf, gold_rlfh))\n",
    "    print(calculate_gap(args, output_normal, gold_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
